<p>
Prosody plays a big role in both direct speech-to-speech translation and cascaded systems. In both the systems prosody modelling has to be done in order to deliver
semantically meaningful sentences in addition to expressing the same emotion in the source utterance. The problem becomes more difficult because prosody cannot be just 
mapped across langauges as different languages have different way of expressing the same emotion in addition to being context dependent. It has been highlighted in 
AMTA2022 where a keynote speaker presented an example from Squid Game (company <i>Translated</i>; tool <i>Matesub</i>), the synthesis sounded unnatural. 
</p>
<p>
Prosody Modelling - 
Controllable speech synthesis - 
</p>
<hr>
<h2>Proposed solution</h2>

<hr>
<h2>Previous work</h2>
<h3>Prosody modelling in TTS</h3>
<p>
<a href='https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414102'>Xu et.al</a> proposes the use of textual embeddings of neighbouring sentences to improve prosody
generation of utterances in a paragraph in E2E fashion without using any explicit prosody features. Cross-utterance context vectors produced by an additional cross-utterance
encoder based on sentence embeddings extracted from pretrained BERT model are used to augment the input to Tactotron2. They also found that prosody can be controlled by 
changing the neighbouring sentences. To show how changing the neighbouring sentences changes the pitch and duration of the tartget snetence, they compared the
  mel-spectrograms of the sentence <i>Tom called Mary</i> where 1) <i>Tom</i> was the keyword, 2) <i>called</i> was the keyword. There was a variation in pitch and
duration of the keywords in the 2 scenarios, indicating change in prosody.
</p>
<p>
In <a href='https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9640518'>Du et.al</a>, they modelled phone-level prosodies with a GMM-based mixture density 
network(MDN) and then extend it for multi-speaker TTS using speaker adaptation transforms of Gaussian means and variances. They also show that they can clone the 
prosodies from a reference speech by sampling prosodies from the Gaussian components that produce the reference prosodies. Their experiments  show that the proposed 
method with GMM-based MDN not only achieves significantly better diversity than using a single Gaussian in both single-speaker and multispeaker TTS, but also provides 
better naturalness. The prosody cloning experiments demonstrate that the prosody similarity of the proposed method with GMM-based MDN is comparable to recent
proposed fine-grained VAE while the target speaker similarity is better. 
<br>
They used MCD as a measure to find out the distance between ground-truth phone-level prososdies and utterance-level prosodies. As expected phone-level prosodies produced
lower MCD as phone-level prossody representation contain more information about phone pronounciation than utterance-leve prosody representation in both single-speaker 
and multi-speaker systems. It was discovered that PLP(phone-level prosody modelling)-GMM produced better prosody diversity than PLP-SG(single gaussian) because a 
sequence of phone-level embeddings depicts the prosody more precisely than an utterance-level embedding Gaussian mixtures can better model the phone-level prosody
embeddings than a single Gaussian. 
<br>
The MUSHRA score of PLP-GMM is 25.3% and 8.6% higher than PLP-SG on single-speaker and multi-speaker task respectively with p-value<0.05. It was found that global prosody
representation provided limited information on phone-level prosody and was too coarse to precisely control the synthesis. Phone-level prosodies were controlled using 
specified Gaussian indices.
</p>
<p>
<a href='https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414413'>Hodari et.al</a> provides an introduction to CAMP to address 1) slower rate of prosody variation,
2)determination of prosody without sufficient context. To mitigate the challenge of modelling a slow-varying signal, prosodic information is learned on word-level
representation(stage 1). To alleviate the ill-posed nature of prosody modelling, syntactic and semantic information derived from text is used to learn a 
contextdependent prior over our prosodic space(stage 2). Replacing attention with a jointly trained duration model helps improve prosody.
<br>
The duration model is trained jointly and shares a phone encoder with the acoustic decoder. Thus, the phone embeddings are influenced by both acoustic and duration
losses, in turn providing richer inputs to the duration model at inference time. The duration model trained in stage-1 uses phone embeddings and disentangled prosody 
representation (upsampled to phone-level) as input. This ensures the representation can capture duration-related information.
</p>
<h3>Controllable speech synthesis</h3>
<p>
<a href='https://arxiv.org/pdf/1810.07217.pdf'>Hsu et.al</a> proposes a neural sequence-to-sequence text-to-speech (TTS) model which can control latent attributes in 
the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is 
formulated as a conditional generative model based on the VAE framework, with two levels of hierarchical latent variables. The first level is a categorical variable, 
which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, 
which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. 
This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Their model is capable of inferring speaker and style attributes from a noisy 
utterance and synthesize clean speech with controllable speaking styles.
</p>
<p>
<a href='https://arxiv.org/pdf/1910.01709.pdf'>Habib et.al</a> provided partial supervision to some of the latent variables and were able to force the latent variables
to take on consistent and interpretable purposes. Their model is able to reliably discover and control important but rarely labelled attributes of speech, such as 
affect and speaking rate, with as little as 1% (30 minutes) supervision and didnot observe any degradation os synthesis comapred to SOTA baselines.
</p>
<p>
<a href='https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9362069'>Li et.al</a>
</p>
<p>
<a href='https://arxiv.org/pdf/2009.06775.pdf'>Raitio et.al</a>
</p>
<p>
  <a href='https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9003829'>Zhu et.al</a>
</p>
<p>
  <a href='https://arxiv.org/pdf/2009.08474.pdf'>Hono et.al</a>
</p>
<p>
<a href='https://www.researchgate.net/profile/Zack-Hodari/publication/327388881_Learning_Interpretable_Control_Dimensions_for_Speech_Synthesis_by_Using_External_Data/links/5d0374ed92851c874c651372/Learning-Interpretable-Control-Dimensions-for-Speech-Synthesis-by-Using-External-Data.pdf'>Hodari et.al</a>

</p>
