<h3>Introduction</h3>
SOTA cascaded system seems to be performing better than SOTA direct speech-to-speech translation system. It can be seen in their respective BLEU scores.
<p>
<table>
  <tr>
    <th>Cascaded system</td>
    <th>Translatotron2</td>
    <th>S2UT reduced + CTC</td>
  </tr>
  <tr>
    <td>43.9</td>
    <td>37.0</td>
    <td>39.9</td>
  </tr>
</table>
</p>
This is solely because current MT systems can translate sentences better than current direct speech-to-speech translation systems in terms of quality of translation.
There are things that need to be dealt with like information loss related to semantics, prosody of the original utterance
<hr>
<h3>My ideas for a possible improvement</h3>
<p> 
In order to reduce the gap between the translation quality in cascaded system and direct S2S system, I would try and disentangle linguistic information from 
speaker information. I would attempt to map the linguistics of the source and target language in the syllable (discrete units) granularity. This could potentially 
reduce the number of parameters required to be mapped and also achieve the level of translation quality compared to an MT system. I can also leverage upon context and 
predict a sequence of syllables(discrete units) given the source language syllable and previous syllables in the target language. This should potentially improve the 
quality of the synthesis in terms of content.
</p>
<hr>
<h3>Previous work</h3>
<p>
Disentagling linguistic content from speaker chracteristics or prosodic information has been attempted before in direct speech-to-speech transaltion systems in order to 
utilise the modelling techniques of MT. Experiemnts in <a href="https://arxiv.org/pdf/2107.05604.pdf">Lee et.al</a>, show that the performance of their direct system can
match the performance of a cascaded system when combining discrete unit prediction with text and speech joint training.
</p>
<p>
Information loss in a direct speech translation system happens because processing audio files requires large datasets compared to relatively small datasets required to
process the corresponding text files. This leads to redundant representation. This is managed as highlighted in <a href='https://arxiv.org/pdf/2109.04574.pdf'>Papi et.al</a>
by collapsing adjacent vectors in a fixed way, i.e. by mapping a predefined number of vectors (usually 4) into a single one, either using strided convolutional layers
or by stacking them. These length reduction solutions does lower input redundacy they disregard the variability over time of the amount of linguistic and phonetic
information in audio signals (e.g. due to pauses and speaking rate variations) by giving equal weight to all features. As a result, relevant features are given equal importance
as irrelevant ones contributing to information loss. 
</p>
<p>
 
</p>
